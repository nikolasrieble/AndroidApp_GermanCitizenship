{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = 'https://www.einbuergerungstest-online.eu/'\n",
    "question_subsite = 'fragen/'\n",
    "pages = [''] + [str(i) for i in np.arange(2,11)] #first pages does not start with '1' but with ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_questions(soup):\n",
    "    mydivs = soup.findAll(\"div\", {\"class\": \"questions-question-text\"})\n",
    "    questions  =[[list(j.children) for j in list(i.children)] for i in mydivs]\n",
    "\n",
    "    clean = []\n",
    "    for q in questions:\n",
    "        #get the link if existing - else set a link to None\n",
    "        try: \n",
    "            qid, qtext = q\n",
    "            qlink = None\n",
    "        except:\n",
    "            qid, qtext, qlink = q\n",
    "            #extract the link\n",
    "            qlink = qlink[0]['href']\n",
    "\n",
    "\n",
    "\n",
    "        #extract the text without the link\n",
    "        if qtext[0].format:\n",
    "            rawtext = qtext[0].extract()\n",
    "        else:\n",
    "            rawtext = qtext[0].text\n",
    "\n",
    "        clean.append((qid, rawtext, qlink))\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answers(soup):\n",
    "    answers = list(soup.findAll(\"li\"))\n",
    "    raw = [list(i.children)[0] for i in answers][35:-12] #get only the answers on the page and ignore the rest\n",
    "\n",
    "    # clear the answers from the green coloring on the website which indicates truth\n",
    "    # add truth indicator as 0 or 1 \n",
    "    clean = []\n",
    "    for content in raw:\n",
    "        if content.format:\n",
    "            clean.append((content.extract(), 0))\n",
    "        else:\n",
    "            clean.append((content.text, 1))   \n",
    "    return np.array(clean[:30*4]).reshape(30,4,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for page in pages:\n",
    "    url_to_scrape = base + question_subsite + page\n",
    "    r = requests.get(url_to_scrape)\n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "    answers.append((get_answers(soup)))\n",
    "    questions.append((get_questions(soup)))\n",
    "\n",
    "    print(page)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
